{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sistema de detección de enlaces spam"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 1: Carga del conjunto de datos desde internet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n",
                        "Index(['url', 'is_spam'], dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Carga del conjunto de datos desde internet\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "# Mostrar primeras filas\n",
                "print(df.head())\n",
                "\n",
                "print(df.columns)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 2: Preprocesa los enlaces"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "import re\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('omw-1.4')\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "\n",
                "# Crea una función para segmentar las URLs en partes según sus signos de puntuación, elimina las stopwords, lematiza, etcétera.\n",
                "\n",
                "def procesar_url(url):\n",
                "    # 1. Separar por signos de puntuación\n",
                "    partes = re.split(r'\\W+', url.lower())  # \\W+ = no palabras (puntos, /, -, etc.)\n",
                "\n",
                "    # 2. Eliminar palabras vacías (stopwords)\n",
                "    stop_words = set(stopwords.words('english'))\n",
                "    partes_filtradas = [p for p in partes if p and p not in stop_words]\n",
                "\n",
                "    # 3. Lematizar\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    lematizadas = [lemmatizer.lemmatize(p) for p in partes_filtradas]\n",
                "\n",
                "    return lematizadas\n",
                "\n",
                "# Aplicar la función a todas las URLs\n",
                "df['processed'] = df['url'].apply(preprocess_url)\n",
                "\n",
                "# Dividir en X (datos) e y (etiquetas)\n",
                "X = df['processed']\n",
                "y = df['is_spam']\n",
                "\n",
                "# Dividir en entrenamiento y prueba\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 3: Construye un SVM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.96      0.97      0.97       455\n",
                        "        True       0.91      0.88      0.89       145\n",
                        "\n",
                        "    accuracy                           0.95       600\n",
                        "   macro avg       0.93      0.92      0.93       600\n",
                        "weighted avg       0.95      0.95      0.95       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "# Crear un pipeline: convierte texto y entrena modelo\n",
                "model = Pipeline([\n",
                "    ('tfidf', TfidfVectorizer()),\n",
                "    ('svm', SVC())\n",
                "])\n",
                "\n",
                "# Entrenar\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Evaluar\n",
                "predictions = model.predict(X_test)\n",
                "print(classification_report(y_test, predictions))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 4: Optimiza el modelo anterior"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mejores parámetros: {'svm__C': 1, 'svm__kernel': 'rbf'}\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.96      0.97      0.97       455\n",
                        "        True       0.91      0.88      0.89       145\n",
                        "\n",
                        "    accuracy                           0.95       600\n",
                        "   macro avg       0.93      0.92      0.93       600\n",
                        "weighted avg       0.95      0.95      0.95       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "# Opciones de búsqueda\n",
                "params = {\n",
                "    'svm__C': [0.1, 1, 10],\n",
                "    'svm__kernel': ['linear', 'rbf']\n",
                "}\n",
                "\n",
                "# Grid Search\n",
                "grid = GridSearchCV(model, params, cv=5)\n",
                "grid.fit(X_train, y_train)\n",
                "\n",
                "# Mejores resultados\n",
                "print(\"Mejores parámetros:\", grid.best_params_)\n",
                "\n",
                "# Evaluar modelo optimizado\n",
                "predictions = grid.predict(X_test)\n",
                "print(classification_report(y_test, predictions))\n",
                "\n",
                "# Guardamos el mejor modelo\n",
                "best_model = grid.best_estimator_\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 5: Guarda el modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Modelo guardado como modelo_spam.pkl\n"
                    ]
                }
            ],
            "source": [
                "import joblib\n",
                "\n",
                "# Guardar modelo\n",
                "joblib.dump(best_model, 'modelo_spam.pkl')\n",
                "print(\"Modelo guardado como modelo_spam.pkl\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
